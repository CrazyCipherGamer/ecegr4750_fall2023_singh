{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV into DataFrame to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age_range</th>\n",
       "      <th>num_haircuts_life</th>\n",
       "      <th>has_tiktok</th>\n",
       "      <th>remembers_disco</th>\n",
       "      <th>uses_skincare</th>\n",
       "      <th>max_annual_earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112213500903.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>100-119</td>\n",
       "      <td>360</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>32890.160162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100_0_0_20170112215240346.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>100-119</td>\n",
       "      <td>627</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>29870.803247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100_1_0_20170110183726390.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>100-119</td>\n",
       "      <td>687</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>62930.622654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100_1_0_20170112213001988.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>100-119</td>\n",
       "      <td>710</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>31105.957009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100_1_0_20170112213303693.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>100-119</td>\n",
       "      <td>614</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>63977.673549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                filename  age  gender   race  \\\n",
       "0           0  100_0_0_20170112213500903.jpg.chip.jpg  100    male  white   \n",
       "1           1  100_0_0_20170112215240346.jpg.chip.jpg  100    male  white   \n",
       "2           2  100_1_0_20170110183726390.jpg.chip.jpg  100  female  white   \n",
       "3           3  100_1_0_20170112213001988.jpg.chip.jpg  100  female  white   \n",
       "4           4  100_1_0_20170112213303693.jpg.chip.jpg  100  female  white   \n",
       "\n",
       "  age_range  num_haircuts_life has_tiktok remembers_disco uses_skincare  \\\n",
       "0   100-119                360         no              no            no   \n",
       "1   100-119                627         no              no            no   \n",
       "2   100-119                687         no             yes            no   \n",
       "3   100-119                710         no              no            no   \n",
       "4   100-119                614         no              no            no   \n",
       "\n",
       "   max_annual_earnings  \n",
       "0         32890.160162  \n",
       "1         29870.803247  \n",
       "2         62930.622654  \n",
       "3         31105.957009  \n",
       "4         63977.673549  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/UTKFaceAugmented.csv')\n",
    "\n",
    "# Printing out dataframe head allows me to do feature selection\n",
    "# and determine how best to prepare the data for model training\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the dataframe, it is evident that either age or age_range should be the target value. Here, I will choose age as the target value as this would then be a regression task compared to a classification task. Furthermore, in accordance with current best practices, features related to race and gender will be removed. Numeric features such as max_annual_earnings and num_haircuts_life are also standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features and standardize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also one-hot encode the data such as has_tiktok, remembers_disco, and uses_skincare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "      <th>has_tiktok</th>\n",
       "      <th>remembers_disco</th>\n",
       "      <th>uses_skincare</th>\n",
       "      <th>num_haircuts_life_st</th>\n",
       "      <th>max_annual_earnings_st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_0_0_20170112213500903.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.225473</td>\n",
       "      <td>-0.353687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_0_0_20170112215240346.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.256254</td>\n",
       "      <td>-0.361172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100_1_0_20170110183726390.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.712609</td>\n",
       "      <td>-0.279210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100_1_0_20170112213001988.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.887545</td>\n",
       "      <td>-0.358110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100_1_0_20170112213303693.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.157377</td>\n",
       "      <td>-0.276614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  age  has_tiktok  remembers_disco  \\\n",
       "0  100_0_0_20170112213500903.jpg.chip.jpg  100           0                0   \n",
       "1  100_0_0_20170112215240346.jpg.chip.jpg  100           0                0   \n",
       "2  100_1_0_20170110183726390.jpg.chip.jpg  100           0                1   \n",
       "3  100_1_0_20170112213001988.jpg.chip.jpg  100           0                0   \n",
       "4  100_1_0_20170112213303693.jpg.chip.jpg  100           0                0   \n",
       "\n",
       "   uses_skincare  num_haircuts_life_st  max_annual_earnings_st  \n",
       "0              0              1.225473               -0.353687  \n",
       "1              0              3.256254               -0.361172  \n",
       "2              0              3.712609               -0.279210  \n",
       "3              0              3.887545               -0.358110  \n",
       "4              0              3.157377               -0.276614  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_drop = ['Unnamed: 0', 'gender', 'race', 'age_range', 'num_haircuts_life', 'max_annual_earnings']\n",
    "\n",
    "# these numerical features are standardized so that the model does not have to\n",
    "# train on features where the values have vastly different ranges.\n",
    "# This helps train the model faster as well\n",
    "df['num_haircuts_life_st'] = utils.standardize_numeric(df['num_haircuts_life'])\n",
    "df['max_annual_earnings_st'] = utils.standardize_numeric(df['max_annual_earnings'])\n",
    "\n",
    "# categorical features are encoded numerically so that they can be used in the models\n",
    "keep_categoric_columns = ['has_tiktok', 'remembers_disco', 'uses_skincare']\n",
    "for col in keep_categoric_columns:\n",
    "    df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "\n",
    "df = df.drop(features_to_drop, axis=1)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (14224, 6) y train: (14224, 1)\n",
      "x val:  (4742, 6) y train: (4742, 1)\n",
      "x test:  (4742, 6) y test: (4742, 1)\n",
      "                                    filename  num_haircuts_life_st  \\\n",
      "8986   28_0_4_20170103213024052.jpg.chip.jpg             -0.242470   \n",
      "2834   20_1_0_20170117141304848.jpg.chip.jpg             -0.957426   \n",
      "2203    1_0_4_20170103210812538.jpg.chip.jpg             -1.474629   \n",
      "3445   22_0_4_20170103234043547.jpg.chip.jpg              0.259521   \n",
      "21774   6_1_3_20161220223052131.jpg.chip.jpg             -1.208422   \n",
      "...                                      ...                   ...   \n",
      "21575  68_0_2_20170116193554465.jpg.chip.jpg              1.126596   \n",
      "5390   25_1_0_20170117152038451.jpg.chip.jpg              0.191068   \n",
      "860    16_0_0_20170110232038257.jpg.chip.jpg             -1.269269   \n",
      "15795  40_1_1_20170113011244319.jpg.chip.jpg              0.632211   \n",
      "23654   9_0_0_20170120133313910.jpg.chip.jpg             -1.109545   \n",
      "\n",
      "       max_annual_earnings_st  has_tiktok  remembers_disco  uses_skincare  \n",
      "8986                -0.076873           1                0              1  \n",
      "2834                -0.180135           1                0              1  \n",
      "2203                -0.435229           0                0              0  \n",
      "3445                -0.124474           1                0              1  \n",
      "21774               -0.435229           0                0              0  \n",
      "...                       ...         ...              ...            ...  \n",
      "21575               -0.256879           1                1              0  \n",
      "5390                -0.266986           0                0              0  \n",
      "860                 -0.430752           0                0              1  \n",
      "15795                0.359569           0                0              1  \n",
      "23654               -0.435229           0                0              0  \n",
      "\n",
      "[14224 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# one of the features has to be filename because it will be used later\n",
    "# to poll the dataloader to stream the appropriate images\n",
    "features = ['filename', 'num_haircuts_life_st',\t'max_annual_earnings_st', \n",
    "            'has_tiktok', 'remembers_disco', 'uses_skincare']\n",
    "target = ['age']\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df[features], df[target], train_size=0.6, random_state=42)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, train_size=0.5, random_state=42)\n",
    "\n",
    "print(\"x train: \",x_train.shape, \"y train:\", y_train.shape)\n",
    "print(\"x val: \",x_val.shape, \"y train:\", y_val.shape)\n",
    "print(\"x test: \",x_test.shape, \"y test:\", y_test.shape)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to torch Tensors and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I separate the numerical data from the filename as it is necessary later on\n",
    "x_train_imagename, x_val_imagename, x_test_imagename = x_train.iloc[:, 0], x_val.iloc[:, 0], x_test.iloc[:, 0]\n",
    "x_train, x_val, x_test = x_train.iloc[:, 1:], x_val.iloc[:, 1:], x_test.iloc[:, 1:]\n",
    "\n",
    "x_train_image = utils.process_images(x_train_imagename)\n",
    "x_val_image = utils.process_images(x_val_imagename)\n",
    "x_test_image = utils.process_images(x_test_imagename)\n",
    "\n",
    "# The data is converted to torch.Tensor so that loading the data is easier\n",
    "x_train, x_val, x_test = torch.Tensor(x_train.to_numpy()), torch.Tensor(x_val.to_numpy()), torch.Tensor(x_test.to_numpy())\n",
    "y_train, y_val, y_test = torch.Tensor(y_train.to_numpy()), torch.Tensor(y_val.to_numpy()), torch.Tensor(y_test.to_numpy())\n",
    "x_train_image, x_val_image, x_test_image = torch.Tensor(x_train_image), torch.Tensor(x_val_image), torch.Tensor(x_test_image)\n",
    "\n",
    "\n",
    "# store it in a dict that we can save out as a single file\n",
    "data_dict = {'x_train':x_train, 'x_val':x_val, 'x_test':x_test, \n",
    "             'y_train':y_train, 'y_val':y_val, 'y_test':y_test}\n",
    "\n",
    "# save it to local data directory\n",
    "torch.save(data_dict, 'data/image_csv_processed.pt')\n",
    "\n",
    "image_dict = {'x_train_image': x_train_image, 'x_val_image': x_val_image, 'x_test_image': x_test_image}\n",
    "\n",
    "torch.save(image_dict, 'data/images_processed.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
